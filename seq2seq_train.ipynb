{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fELG9vDJuJdc"
   },
   "source": [
    "# seq2seq: Train\n",
    "\n",
    "Abdulhakim Alnuqaydan, Ali Kadhim, Sergei Gleyzer, Harrison Prosper\n",
    "\n",
    "July 2021\n",
    "\n",
    "## Description\n",
    "\n",
    "Use an encoder/decoder model built using LSTMs to map symbolic mathematical expressions $f(x)$ to their Taylor series expansions to ${\\cal O}(x^5)$.\n",
    "\n",
    "We've heavily borrowed from Charon Guo's excellent tutorial at:\n",
    "\n",
    "https://charon.me/posts/pytorch/pytorch_seq2seq_1/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WfyAPbvu4l3f"
   },
   "source": [
    "### Using Google Colaboratory\n",
    "If you want to use Google colab then uncomment the instructions in the next cell. When that cell is executed, you'll be directed to a Google sign-in page. Once signed in, copy the validation code into the entry window on this page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QF9IVbkY-rJF"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26153,
     "status": "ok",
     "timestamp": 1626115804950,
     "user": {
      "displayName": "Harrison Prosper",
      "photoUrl": "",
      "userId": "06677011918885244894"
     },
     "user_tz": -120
    },
    "id": "qTWYqrekuJde",
    "outputId": "ff752858-02d3-49a3-bb84-149787f63ce8"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive \n",
    "#drive.mount('/content/gdrive')\n",
    "#import sys\n",
    "#sys.path.append('/content/gdrive/My Drive/AI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 5024,
     "status": "ok",
     "timestamp": 1626115816416,
     "user": {
      "displayName": "Harrison Prosper",
      "photoUrl": "",
      "userId": "06677011918885244894"
     },
     "user_tz": -120
    },
    "id": "GwFXx5YluJde"
   },
   "outputs": [],
   "source": [
    "# symbolic mathematics\n",
    "import sympy as sp\n",
    "from sympy import exp, \\\n",
    "    cos, sin, tan, \\\n",
    "    cosh, sinh, tanh, ln, log, E\n",
    "\n",
    "x = sp.Symbol('x')\n",
    "\n",
    "# array manipulation\n",
    "import numpy as np\n",
    "import random as rn\n",
    "import math\n",
    "import time\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "\n",
    "# custom data prep and loader\n",
    "from seq2sequtil import loadData, \\\n",
    "Seq2SeqDataPreparer, Seq2SeqDataLoader\n",
    "\n",
    "# enable pretty printing of symbolic equations\n",
    "from IPython.display import display\n",
    "sp.init_printing(use_latex='mathjax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1626115818752,
     "user": {
      "displayName": "Harrison Prosper",
      "photoUrl": "",
      "userId": "06677011918885244894"
     },
     "user_tz": -120
    },
    "id": "EvmoUxfYuJdf"
   },
   "outputs": [],
   "source": [
    "#BASE = '/content/gdrive/My Drive/AI'\n",
    "BASE = '.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_rfrZXDE4l3n"
   },
   "source": [
    "Duration of an epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 385,
     "status": "ok",
     "timestamp": 1626115821978,
     "user": {
      "displayName": "Harrison Prosper",
      "photoUrl": "",
      "userId": "06677011918885244894"
     },
     "user_tz": -120
    },
    "id": "xOUoa7pT4l3n"
   },
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qjoD0FFjEHSc"
   },
   "source": [
    "Function to count the number of parameters in a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1626115823969,
     "user": {
      "displayName": "Harrison Prosper",
      "photoUrl": "",
      "userId": "06677011918885244894"
     },
     "user_tz": -120
    },
    "id": "riLbeY85EF5r"
   },
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gur9S9MWuJdi"
   },
   "source": [
    "### Load sequence data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2549,
     "status": "ok",
     "timestamp": 1626115831006,
     "user": {
      "displayName": "Harrison Prosper",
      "photoUrl": "",
      "userId": "06677011918885244894"
     },
     "user_tz": -120
    },
    "id": "TvB9KSRA4l3h",
    "outputId": "c49bfef7-e8aa-45c6-b5cb-2cb76506d0c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example source:\n",
      "\t(4*x**2-1)*cos(2*x+9)/cosh(7*x**3/7)/(8*x-8)*exp(-5*x**2+1)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{\\left(4 x^{2} - 1\\right) e^{1 - 5 x^{2}} \\cos{\\left(2 x + 9 \\right)}}{\\left(8 x - 8\\right) \\cosh{\\left(x^{3} \\right)}}$"
      ],
      "text/plain": [
       "                   2             \n",
       "⎛   2    ⎞  1 - 5⋅x              \n",
       "⎝4⋅x  - 1⎠⋅ℯ        ⋅cos(2⋅x + 9)\n",
       "─────────────────────────────────\n",
       "                      ⎛ 3⎞       \n",
       "        (8⋅x - 8)⋅cosh⎝x ⎠       "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example target:\n",
      "\tE*cos(9)/8+x*(E*cos(9)/8-E*sin(9)/4)+x**2*(-E*sin(9)/4-5*E*cos(9)/4)+x**3*(13*E*sin(9)/6-5*E*cos(9)/4)+x**4*(247*E*cos(9)/48+13*E*sin(9)/6)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle x^{4} \\left(\\frac{247 e \\cos{\\left(9 \\right)}}{48} + \\frac{13 e \\sin{\\left(9 \\right)}}{6}\\right) + x^{3} \\left(\\frac{13 e \\sin{\\left(9 \\right)}}{6} - \\frac{5 e \\cos{\\left(9 \\right)}}{4}\\right) + x^{2} \\left(- \\frac{e \\sin{\\left(9 \\right)}}{4} - \\frac{5 e \\cos{\\left(9 \\right)}}{4}\\right) + x \\left(\\frac{e \\cos{\\left(9 \\right)}}{8} - \\frac{e \\sin{\\left(9 \\right)}}{4}\\right) + \\frac{e \\cos{\\left(9 \\right)}}{8}$"
      ],
      "text/plain": [
       " 4 ⎛247⋅ℯ⋅cos(9)   13⋅ℯ⋅sin(9)⎞    3 ⎛13⋅ℯ⋅sin(9)   5⋅ℯ⋅cos(9)⎞    2 ⎛  ℯ⋅sin(\n",
       "x ⋅⎜──────────── + ───────────⎟ + x ⋅⎜─────────── - ──────────⎟ + x ⋅⎜- ──────\n",
       "   ⎝     48             6     ⎠      ⎝     6            4     ⎠      ⎝     4  \n",
       "\n",
       "9)   5⋅ℯ⋅cos(9)⎞     ⎛ℯ⋅cos(9)   ℯ⋅sin(9)⎞   ℯ⋅cos(9)\n",
       "── - ──────────⎟ + x⋅⎜──────── - ────────⎟ + ────────\n",
       "         4     ⎠     ⎝   8          4    ⎠      8    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inputs, targets = loadData('data/seq2seq_data_10000.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert sequence data\n",
    "  1. Scan input and output sequences and construct maps of characters (tokens) to indices, one map for input sequences and another for target sequences.\n",
    "  1. Pad sequences to the same length\n",
    "  1. Split into train, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of seq-pairs (train):     8000\n",
      "number of seq-pairs (valid):     1000\n",
      "number of seq-pairs (test):      1000\n",
      "\n",
      "number of source tokens:           31\n",
      "max source sequence length:        81\n",
      "\n",
      "number of target tokens:           35\n",
      "max target sequence length:       923\n"
     ]
    }
   ],
   "source": [
    "import seq2sequtil as sq\n",
    "import importlib \n",
    "importlib.reload(sq)\n",
    "\n",
    "fractions=[8/10, 9/10]\n",
    "\n",
    "db = sq.Seq2SeqDataPreparer(inputs, targets, fractions)\n",
    "\n",
    "print(db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TjCtDW324l3i"
   },
   "source": [
    "## Implement encoder\n",
    "\n",
    "### LSTM \n",
    "\n",
    "An LSTM is a function that is typically conceptualized as a \"device\" containing various \"gates\" that filter input data in different ways. This creative conceptual reasoning has yielded functions with amazing capabilities. However, it is far from clear that this approach will be the way forward in the future. Why? Because we have compelling evidence that highly creative conceptual reasoning, while impressive, is not, actually, needed to arrive at functions with immense capability. The existence of human brains that have evolved through natural selection is an existence proof that immensely capable functions can be arrived at through trial and error.\n",
    "\n",
    "No doubt, one day, someone will devise an effective evolutionary algorithm that will compress millions of years of evolution into a matter of weeks or even days in order to construct immensely capable functions that could far outstrip what could be done through reasoning. \n",
    "\n",
    "#### LSTM Function\n",
    "\n",
    "At time step $t$, instances of the LSTM class in PyTorch compute the function\n",
    "\n",
    "\\begin{align*}\n",
    " i_t & = \\sigma(W_{ii} x_t + b_{ii} + W_{hi} h_{t-1} + b_{hi}),\\\\\n",
    " f_t & = \\sigma(W_{if} x_t + b_{if} + W_{hf} h_{t-1} + b_{hf}),\\\\\n",
    " g_t & = \\tanh(W_{ig} x_t + b_{ig} + W_{hg} h_{t-1} + b_{hg}),\\\\ \\\\\n",
    " c_t & = f_t \\odot c_{t-1} + i_t \\odot g_t,\\\\\n",
    " o_t & = \\sigma(W_{io} x_t + b_{io} + W_{ho} h_{t-1} + b_{ho}),\\\\\n",
    " h_t & = o_t \\odot \\tanh(c_t),\n",
    "\\end{align*}\n",
    "\n",
    "where $\\sigma$ is a sigmoid and $\\odot$ is the Hadamard product. The functions $i_t$, and $f_t$ are called the *input* and *forget* gates, respectively, while $c_t$ and $h_t$ are called the *cell* and *hidden* states, respectively, of the LSTM. The cell and hidden states $c_{t-1}$ and $h_{t-1}$ are from the previous time step. The number of elements that comprise the cell and hidden states is specified by the *hidden_size* argument of the PyTorch LSTM. The $W$s and $b$s are the parameters of the LSTM function. The LSTM outputs $o_t, (h_t, c_t)$.\n",
    "\n",
    "### PyTorch LSTM Parameters\n",
    "\n",
    "* input_size – The number of expected features in the input x\n",
    "* hidden_size – The number of features in the hidden state h\n",
    "* num_layers – Number of recurrent layers. E.g., setting num_layers=2 would mean stacking two LSTMs together to form a stacked LSTM, with the second LSTM taking in outputs of the first LSTM and computing the final results. Default: 1\n",
    "* bias – If False, then the layer does not use bias weights b_ih and b_hh. Default: True\n",
    "* batch_first – If True, then the input and output tensors are provided as (batch, seq, feature) instead of (seq, batch, feature). Note that this does not apply to hidden or cell states. See the Inputs/Outputs sections below for details. Default: False\n",
    "* dropout – If non-zero, introduces a Dropout layer on the outputs of each LSTM layer except the last layer, with dropout probability equal to dropout. Default: 0\n",
    "* bidirectional – If True, becomes a bidirectional LSTM. Default: False\n",
    "* proj_size – If > 0, will use LSTM with projections of corresponding size. Default: 0\n",
    "\n",
    "### Encoder\n",
    "  * num_features - Number of unique source tokens\n",
    "  * embed_size - Dimension of embedding space for tokens\n",
    "  * hidden_size - See above\n",
    "  * number_layers - See above\n",
    "  * dropout - See above\n",
    "  * device - device on which to do calculations\n",
    "  \n",
    "### Decoder\n",
    "  * num_features - Number of unique source tokens\n",
    "  * embed_size - Dimension of embedding space for tokens\n",
    "  * hidden_size - See above\n",
    "  * number_layers - See above\n",
    "  * dropout - See above\n",
    "  * device - device on which to do calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda\n"
     ]
    }
   ],
   "source": [
    "HIDDEN_SIZE = 64   # size of vectors holding hidden and cell states\n",
    "NUM_LAYERS  =  8   # number of LSTM layers\n",
    "EOS         = db.y_token2index['\\n']\n",
    "PADDING     = db.y_token2index[' ']\n",
    "DEVICE      = torch.device(\"cuda\" \\\n",
    "                           if torch.cuda.is_available() \\\n",
    "                           else \"cpu\")\n",
    "print('DEVICE:', DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 615,
     "status": "ok",
     "timestamp": 1626123088610,
     "user": {
      "displayName": "Harrison Prosper",
      "photoUrl": "",
      "userId": "06677011918885244894"
     },
     "user_tz": -120
    },
    "id": "JdD6YxTN4l3j"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_features, \n",
    "                 embed_size=10, \n",
    "                 hidden_size=HIDDEN_SIZE, \n",
    "                 num_layers=NUM_LAYERS, \n",
    "                 dropout=0.5,\n",
    "                 device=DEVICE):\n",
    "        \n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.num_features= num_features\n",
    "        self.embed_size  = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers  = num_layers\n",
    "        self.dropout     = dropout\n",
    "        \n",
    "        # The embedding will map each index associated with a token \n",
    "        # into a vector of dimension embed_size, where typically, \n",
    "        # in natural language translation tasks \n",
    "        # embed_size << num_features. In this example that is not the\n",
    "        # case.\n",
    "        #\n",
    "        # The shape of the inputs to embedding is \n",
    "        #  (sequence_length, batch_size).\n",
    "        #\n",
    "        # The shape of the outputs is \n",
    "        #  (sequence_lenggth, batch_size, embed_size)\n",
    "        self.embedding = nn.Embedding(num_features, \n",
    "                                      embed_size).to(device)\n",
    "        \n",
    "        # By default, LSTM expects the input shape to be\n",
    "        #  (seq_len, batch_size, embed_size) (seq_len: sequence_length)\n",
    "        self.lstm = nn.LSTM(embed_size, \n",
    "                            hidden_size, \n",
    "                            num_layers,\n",
    "                            dropout=dropout).to(device)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x.shape: (seq_len, batch_size)\n",
    "       \n",
    "        x = self.embedding(x)\n",
    "        # x.shape: (seq_len, batch_size, embed_size)\n",
    "        \n",
    "        output, (hidden, cell) = self.lstm(x)\n",
    "        # output.shape: (seq_len, batch_size, hidden_size)\n",
    "\n",
    "        # we discard output of encoder\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q6SStaRM4l3j"
   },
   "source": [
    "## Implement Decoder\n",
    "\n",
    "The decoder is similar to the encoder in that it has an embedding layer that maps the target features (the indices associated with the characters in the target sequences) to a vector in an embedding space. \n",
    "\n",
    "The key difference is that the output of the LSTM is passed to a *Linear* layer that outputs a vector equal in size to the number of features, i.e., unique tokens associated with the target sequences. \n",
    "\n",
    "Starting with the start of sequence (SOS) token (here a tab), the decoder outputs a vector of floats each corresponding to a target token. The ordinal value of the largest float is the decoder's prediction of the next token in the target sequence. The prediction forms the input token for the next call to the decoder. The decoder is called repeatedly until it predicts an end of sequence (EOS) token (here a newline) or the maximum target sequence length is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 365,
     "status": "ok",
     "timestamp": 1626123091084,
     "user": {
      "displayName": "Harrison Prosper",
      "photoUrl": "",
      "userId": "06677011918885244894"
     },
     "user_tz": -120
    },
    "id": "uFyw9rHu4l3j"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    '''\n",
    "    num_features number of target features, i.e., tokens\n",
    "    '''\n",
    "    def __init__(self, num_features, \n",
    "                 embed_size=10, \n",
    "                 hidden_size=HIDDEN_SIZE, \n",
    "                 num_layers=NUM_LAYERS, \n",
    "                 dropout=0.5, \n",
    "                 device=DEVICE):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.num_features= num_features  # number of target tokens\n",
    "        self.embed_size  = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers  = num_layers\n",
    "        self.dropout     = dropout\n",
    "        self.device      = device\n",
    " \n",
    "        # The shape of inputs must be (seq_len, batch_size)\n",
    "        # The shape of outputs is (seq_len, batch_size, embed_size)\n",
    "        self.embedding   = nn.Embedding(num_features, \n",
    "                                        embed_size).to(device)\n",
    "\n",
    "        # inputs.shape:  (seq_len, batch_size, embed_size)\n",
    "        # outputs.shape: (seq_len, batch_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(input_size  = embed_size, \n",
    "                            hidden_size = hidden_size, \n",
    "                            num_layers  = num_layers,\n",
    "                            dropout     = dropout).to(device)\n",
    "\n",
    "        # inputs.shape:  (batch_size, hidden_size)\n",
    "        # outputs.shape: (batch_size, num_features)\n",
    "        self.linear = nn.Linear(hidden_size, num_features).to(device)\n",
    "\n",
    "    def forward(self, source, hidden, cell):\n",
    "        # For a given batch instance, the indices of tokens will be \n",
    "        # passed one by one to this function. Therefore, the source\n",
    "        # is a 1D tensor of token indices of shape (batch_size).\n",
    "        # But since the embedding object requires an input with shape \n",
    "        # (seq_len, batch_size) we need to use unsqueeze(0) to \n",
    "        # insert an extra dimension of size 1 at dim=0 so that the\n",
    "        # shape is (1, batch_size).\n",
    "        \n",
    "        source = source.unsqueeze(0)\n",
    "        # source.shape:   (1, batch_size)\n",
    "        \n",
    "        # Map each index in the batch of indices to its embedded \n",
    "        # representation.\n",
    "        embedded = self.embedding(source)\n",
    "        # embedded.shape: (1, batch_size, embed_size)\n",
    "                \n",
    "        # Input embedded representation of token and previous\n",
    "        # hidden and cell states to lstm\n",
    "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "        # output.shape:   (1, batch_size, hidden_size)\n",
    "        \n",
    "        # Get rid of dim 0 of length \"1\" to match shape expected by\n",
    "        # the linear layer\n",
    "        output = output.squeeze(0) \n",
    "        # output.shape:   (batch_size, hidden_size)\n",
    "        \n",
    "        prediction = self.linear(output)\n",
    "        # prediction.shape: (batch_size, num_features)\n",
    "        \n",
    "        # We don't need to apply a softmax to prediction because \n",
    "        # this is done by the PyTorch cross entropy class. \n",
    "        # However, we may want to do so at some point, if we write\n",
    "        # our own custom average loss function.\n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jhwo7q1Z4l3k"
   },
   "source": [
    "### Construct seq2seq model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1626123093159,
     "user": {
      "displayName": "Harrison Prosper",
      "photoUrl": "",
      "userId": "06677011918885244894"
     },
     "user_tz": -120
    },
    "id": "Q35uffAu4l3k"
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    '''\n",
    "    model = Model(encoder, decoder, device)\n",
    "    '''\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device  = device\n",
    "        self.num_features = self.decoder.num_features\n",
    "        \n",
    "        assert encoder.hidden_size == decoder.hidden_size, \\\n",
    "            \"hidden_size of encoder and decoder must be equal!\"\n",
    "        \n",
    "        assert encoder.num_layers == decoder.num_layers, \\\n",
    "            \"num_layers of encoder and decoder must be equal!\"\n",
    "        \n",
    "    def forward(self, source, target):\n",
    "        '''\n",
    "    1.  If target is a tuple, then it is interpreted as:\n",
    "        target = (target, teacher_prob)\n",
    "            \n",
    "    2.  If the target is not a tuple, it is assumed to be the target\n",
    "        and the teacher_prob is set to 0.5\n",
    "        '''\n",
    "\n",
    "        if type(target) == type(()):\n",
    "            target, teacher_prob = target\n",
    "        else:\n",
    "            teacher_prob = 0.5\n",
    "            \n",
    "        y_seq_len, batch_size  = target.shape\n",
    "            \n",
    "        # source.shape = (x_seq_len, batch_size)\n",
    "        # target.shape = (y_seq_len, batch_size)\n",
    "        #\n",
    "        # teacher_prob is the probability, during training, to \n",
    "        # use the true target rather than the predicted target.\n",
    "        # ideally that probability should be gradually reduced \n",
    "        # as the training progresses.\n",
    "           \n",
    "        # Tensor to store decoder outputs\n",
    "        outputs = torch.zeros(y_seq_len, \n",
    "                              batch_size, \n",
    "                              self.num_features, \n",
    "                              device=self.device)\n",
    "        \n",
    "        # Use last hidden state of the encoder as the initial \n",
    "        # hidden state of the decoder. (Note: the encoder discards\n",
    "        # the output of the LSTM.)\n",
    "        hidden, cell = self.encoder(source)\n",
    "        \n",
    "        # Repeatedly call decoder to have it predict which target\n",
    "        # token should come next.\n",
    "        #\n",
    "        # The first input to the decoder should be the index \n",
    "        # associated with the tab token of the target sequence\n",
    "        index = target[0,:]\n",
    "        \n",
    "        for t in range(1, y_seq_len):\n",
    "            \n",
    "            output, hidden, cell = self.decoder(index, hidden, cell)\n",
    "            \n",
    "            # Cache predictions for each token\n",
    "            # output.shape: (batch_size, num_features)\n",
    "            # Note: following does a deepcopy of output to outputs[t]\n",
    "            outputs[t] = output\n",
    "                       \n",
    "            # The prediction is the ordinal value of the feature with \n",
    "            # the largest value\n",
    "            prediction = output.argmax(1) \n",
    "            \n",
    "            # For the next index, decide whether to use the \n",
    "            # target or the prediction \n",
    "            if self.train:\n",
    "                use_target = rn.random() < teacher_prob\n",
    "                index = target[t] if use_target else prediction\n",
    "            else:\n",
    "                index = prediction   \n",
    "                            \n",
    "        # Note: For a given batch instance, the num_features outputs\n",
    "        # do not sum to unity.\n",
    "        return outputs\n",
    "    \n",
    "    def predict(self, source, y_max_seq_len=1000):\n",
    "        # source.shape = (x_seq_len)\n",
    "        \n",
    "        SOS = source[0]   # start of sequence code\n",
    "        EOS = source[-1]  # end of sequence code\n",
    "        \n",
    "        # Tensor to store decoder predictions\n",
    "        predictions = torch.zeros(y_max_seq_len,\n",
    "                                  dtype=torch.long,\n",
    "                                  device=self.device)\n",
    "        \n",
    "        # Use last hidden state of the encoder as the initial \n",
    "        # hidden state of the decoder.\n",
    "        \n",
    "        # encoder expects input of shape (seq_len, batch_size)\n",
    "        source = source.unsqueeze(1)\n",
    "        # source.shape = (x_seq_len, 1)\n",
    "        \n",
    "        hidden, cell = self.encoder(source)\n",
    "        \n",
    "        # Repeatedly call decoder to get target sequence.\n",
    "        # The first input to the decoder should be the index \n",
    "        # associated with the tab token. Here we assume that the\n",
    "        # source and target tab and newline codes are the same!\n",
    "        index = source[0, :]\n",
    "        count = 0\n",
    "        \n",
    "        for t in range(1, y_max_seq_len):\n",
    "            count += 1\n",
    "            \n",
    "            output, hidden, cell = self.decoder(index, hidden, cell)\n",
    "            # output.shape: (1, num_features)\n",
    "            \n",
    "            # Predict next token by returning the ordinal value\n",
    "            # of the output \"feature\" with the largest value\n",
    "            index = output.argmax(1) \n",
    "            if index == EOS:\n",
    "                break\n",
    "                \n",
    "            # cache prediction\n",
    "            predictions[t-1] = index\n",
    "        \n",
    "        # send results to CPU\n",
    "        return predictions[:count].cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1yD5JTRC4l3k"
   },
   "source": [
    "### Train Model\n",
    "\n",
    "  1. Create an encoder\n",
    "  1. Create a decoder\n",
    "  1. Create a encoder/decoder model\n",
    "  1. Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1626123094274,
     "user": {
      "displayName": "Harrison Prosper",
      "photoUrl": "",
      "userId": "06677011918885244894"
     },
     "user_tz": -120
    },
    "id": "LZ0sDFXH4l3l",
    "outputId": "171f84ed-cd48-4a28-fa8a-a13b391d97f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(31, 10)\n",
      "    (lstm): LSTM(10, 64, num_layers=8, dropout=0.5)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (embedding): Embedding(35, 10)\n",
      "    (lstm): LSTM(10, 64, num_layers=8, dropout=0.5)\n",
      "    (linear): Linear(in_features=64, out_features=35, bias=True)\n",
      "  )\n",
      ")\n",
      "Computational device:      cuda\n",
      "Number of free parameters: 507767\n"
     ]
    }
   ],
   "source": [
    "num_source_features = db.num_tokens('source')\n",
    "encoder_embed_size  = 10\n",
    "encoder_dropout     = 0.5\n",
    "\n",
    "num_target_features = db.num_tokens('target')\n",
    "decoder_embed_size  = 10\n",
    "decoder_dropout     = 0.5\n",
    "\n",
    "# in this model, the following structural parameters are the same for\n",
    "# both the encoder and decoder. \n",
    "hidden_size         = HIDDEN_SIZE  # size of hidden and cell state vectors\n",
    "num_layers          = NUM_LAYERS   # number of stacked LSTMs\n",
    "\n",
    "encoder = Encoder(num_source_features, \n",
    "                  encoder_embed_size, \n",
    "                  hidden_size, \n",
    "                  num_layers, \n",
    "                  encoder_dropout).to(DEVICE)\n",
    "\n",
    "decoder = Decoder(num_target_features, \n",
    "                  decoder_embed_size, \n",
    "                  hidden_size, \n",
    "                  num_layers, \n",
    "                  decoder_dropout).to(DEVICE)\n",
    "\n",
    "model = Model(encoder, decoder, DEVICE).to(DEVICE)\n",
    "print(model)\n",
    "print('Computational device:      %s' % DEVICE)\n",
    "print('Number of free parameters: %d' % count_parameters(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nv6qIvR34l3l"
   },
   "source": [
    "### Choose optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "executionInfo": {
     "elapsed": 565,
     "status": "ok",
     "timestamp": 1626123106056,
     "user": {
      "displayName": "Harrison Prosper",
      "photoUrl": "",
      "userId": "06677011918885244894"
     },
     "user_tz": -120
    },
    "id": "d1eEV5Am4l3l"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8FJGemwi4l3l"
   },
   "source": [
    "### Choose loss function\n",
    "For each token with associated index k, compute the cross-entropy loss\n",
    "\n",
    "\\begin{align*}\n",
    "    E_k & = -\\log \\left( \\frac{\\exp(\\hat{y}_k)}{\\sum_{\\{j\\}} \\exp(\\hat{y}_j) } \\right) ,\n",
    "\\end{align*}\n",
    "\n",
    "where $\\hat{y}_j$ is the $j^\\text{th}$ element of the model's output vector of length *num_features*. The losses $E_k$ are averaged over all tokens in a sequence and all batch instances. The set $\\{ j \\}$  excludes tokens that correspond to padding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1626123108051,
     "user": {
      "displayName": "Harrison Prosper",
      "photoUrl": "",
      "userId": "06677011918885244894"
     },
     "user_tz": -120
    },
    "id": "C0HLgdIl4l3l"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=PADDING).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TXchIB4x4l3l"
   },
   "source": [
    "### Define trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1626123109706,
     "user": {
      "displayName": "Harrison Prosper",
      "photoUrl": "",
      "userId": "06677011918885244894"
     },
     "user_tz": -120
    },
    "id": "Uu0qrPWX4l3m"
   },
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, criterion, device, \n",
    "          clip, n_samples=100):\n",
    "    \n",
    "    # set to train mode\n",
    "    model.train()  \n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    # X and Y are created directly on the device, \n",
    "    # which is most likely a GPU\n",
    "    \n",
    "    dataloader.reset()\n",
    "    for i, (X, Y) in enumerate(dataloader):\n",
    "        \n",
    "        # zero gradients of all trainable parameters\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # compute output of model\n",
    "        output = model(X, Y)\n",
    "        # output.shape: (y_seq_len, batch_size, num_features)\n",
    "    \n",
    "        # skip first token (a tab), then reshape to\n",
    "        # ((y_seq_len-1)*batch_size, num_features)\n",
    "        num_features = output.shape[-1]\n",
    "        output = output[1:].view(-1, num_features)\n",
    "\n",
    "        # skip first token (a tab), then reshape to\n",
    "        # ((max_target_seq_len-1)*batch_size)\n",
    "        Y = Y[1:].view(-1)\n",
    "               \n",
    "        # average loss over batches\n",
    "        loss = criterion(output, Y)\n",
    "        \n",
    "        # compute gradients using automatic differentiation\n",
    "        loss.backward()\n",
    "        \n",
    "        # clip gradients so they don't blow up\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        # make one step in th model parameter space\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item() # to CPU, if using a GPU\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print('%d ' % i, end='')\n",
    "            \n",
    "        if i + 1 >= n_samples: break\n",
    "            \n",
    "    print('end epoch')\n",
    "    return epoch_loss / (i+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "voIKn--S4l3m"
   },
   "source": [
    "### Evaluator\n",
    "\n",
    "In evaluation mode, we turn off gradient calculation and we provide only the source sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1626123111655,
     "user": {
      "displayName": "Harrison Prosper",
      "photoUrl": "",
      "userId": "06677011918885244894"
     },
     "user_tz": -120
    },
    "id": "03jnoaBr4l3m"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, criterion, device, n_samples=100):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "   \n",
    "    # no need to compute gradients\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        dataloader.reset()\n",
    "        \n",
    "        for i, (X, Y) in enumerate(dataloader):\n",
    "                        \n",
    "            output = model(X, Y)\n",
    "            # output.shape:\n",
    "            # (y_seq_len, batch_size, num_features)\n",
    "\n",
    "            # skip first token (a tab), then reshape to\n",
    "            # ((y_seq_len-1)*batch_size, num_features)\n",
    "            \n",
    "            num_features = output.shape[-1]\n",
    "            \n",
    "            output = output[1:].view(-1, num_features)\n",
    "                            \n",
    "            # skip first token (a tab), then reshape targets to\n",
    "            # ((y_seq_len-1)*batch_size)\n",
    "            Y = Y[1:].view(-1)\n",
    "               \n",
    "            # average loss over mini-batches, excluding padding tokens\n",
    "            loss = criterion(output, Y)\n",
    "        \n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            if i + 1 >= n_samples: break\n",
    "                \n",
    "    return epoch_loss / (i + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "47irOA8H4l3n"
   },
   "source": [
    "### Finally, train!\n",
    "\n",
    "\n",
    "Note: The class __Seq2SeqDataLoader__ is a custom dataloader that returns batches of sequence pairs comprising sources and targets, X and Y, respectively. The quantities X and Y are 2D tensors, each with shape (max_seq_len, batch_size) that comprise sequences of indices arranged in columns. Each index corresponds to a token, i.e., a character. The column lengths, max_seq_len, differ between X and Y.\n",
    "\n",
    "\n",
    "__NB:__ At present, Python iterators can't be reset, so we need to remake them each time to ensure each starts at the beginning. We do so by calling the __reset()__ method of __Seq2SeqDataLoader__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "executionInfo": {
     "elapsed": 494,
     "status": "ok",
     "timestamp": 1626123114625,
     "user": {
      "displayName": "Harrison Prosper",
      "photoUrl": "",
      "userId": "06677011918885244894"
     },
     "user_tz": -120
    },
    "id": "al20iuoOJljr"
   },
   "outputs": [],
   "source": [
    "#=================================================================\n",
    "N_EPOCHS =  2  # number of times through training data\n",
    "CLIP     =  1 # prevent gradients from exploding!\n",
    "#=================================================================\n",
    "train_dataloader = Seq2SeqDataLoader(db.train_data, device=DEVICE)\n",
    "valid_dataloader = Seq2SeqDataLoader(db.valid_data, device=DEVICE)\n",
    "test_dataloader  = Seq2SeqDataLoader(db.test_data,  device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ruLv1WxG4l3n",
    "outputId": "e0e288b5-6237-4e84-d48d-cf5021e09217"
   },
   "outputs": [],
   "source": [
    "def train_me(n_epochs=N_EPOCHS):\n",
    "    best_valid_loss = float('inf')\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        print('epoch: %5d' % epoch)\n",
    "    \n",
    "        start_time = time.time()\n",
    "    \n",
    "        train_loss = train(model, train_dataloader, optimizer, \n",
    "                           criterion, DEVICE, CLIP)\n",
    "        \n",
    "        valid_loss = evaluate(model, valid_dataloader, \n",
    "                              criterion, DEVICE)\n",
    "    \n",
    "        end_time   = time.time()\n",
    "        mins, secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "        if valid_loss < best_valid_loss:\n",
    "            # save best model so far\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(),\n",
    "                       '%s/seq2seq_model_1.pt' % BASE)\n",
    "    \n",
    "        print(' duration(mm:ss): %2.2d:%2.2d | train_loss: %7.4f |'\\\n",
    "              ' valid_loss: %7.4f\\n' % \\\n",
    "              (mins, secs, train_loss, valid_loss))\n",
    "    print('\\ndone!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:     0\n",
      "0 10 20 30 40 50 60 70 80 90 end epoch\n",
      " duration(mm:ss): 00:23 | train_loss:  3.0666 | valid_loss:  3.0457\n",
      "\n",
      "epoch:     1\n",
      "0 10 20 30 40 50 60 70 80 90 end epoch\n",
      " duration(mm:ss): 00:21 | train_loss:  3.0486 | valid_loss:  3.0030\n",
      "\n",
      "\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "train_me()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uo6Wp4Wk4l3n"
   },
   "source": [
    "### Now test!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 244
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "error",
     "timestamp": 1626105849454,
     "user": {
      "displayName": "Harrison Prosper",
      "photoUrl": "",
      "userId": "06677011918885244894"
     },
     "user_tz": -120
    },
    "id": "ZAelscAw4l3n",
    "outputId": "bc4cd2f1-d297-45eb-b53e-9d98324814ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss:  3.0376\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('%s/seq2seq_model_1.pt' % BASE))\n",
    "test_loss = evaluate(model, test_dataloader, criterion, DEVICE)\n",
    "print('test loss: %7.4f' % test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "seq2seq_train.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 - AI",
   "language": "python",
   "name": "python3-ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
