{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fELG9vDJuJdc"
   },
   "source": [
    "# seq2seq: Train\n",
    "\n",
    "Abdulhakim Alnuqaydan, Ali Kadhim, Sergei Gleyzer, Harrison Prosper\n",
    "\n",
    "July 2021\n",
    "\n",
    "## Description\n",
    "\n",
    "Use an encoder/decoder model built using LSTMs to map symbolic mathematical expressions $f(x)$ to their Taylor series expansions to ${\\cal O}(x^5)$.\n",
    "\n",
    "We've heavily borrowed from Charon Guo's excellent tutorial at:\n",
    "\n",
    "https://charon.me/posts/pytorch/pytorch_seq2seq_1/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WfyAPbvu4l3f"
   },
   "source": [
    "### Using Google Colaboratory\n",
    "If you want to use Google colab then uncomment the instructions in the next cell. When that cell is executed, you'll be directed to a Google sign-in page. Once signed in, copy the validation code into the entry window on this page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QF9IVbkY-rJF"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26153,
     "status": "ok",
     "timestamp": 1626115804950,
     "user": {
      "displayName": "Harrison Prosper",
      "photoUrl": "",
      "userId": "06677011918885244894"
     },
     "user_tz": -120
    },
    "id": "qTWYqrekuJde",
    "outputId": "ff752858-02d3-49a3-bb84-149787f63ce8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive \n",
    "drive.mount('/content/gdrive')\n",
    "import sys\n",
    "sys.path.append('/content/gdrive/My Drive/AI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 5024,
     "status": "ok",
     "timestamp": 1626115816416,
     "user": {
      "displayName": "Harrison Prosper",
      "photoUrl": "",
      "userId": "06677011918885244894"
     },
     "user_tz": -120
    },
    "id": "GwFXx5YluJde"
   },
   "outputs": [],
   "source": [
    "# symbolic mathematics\n",
    "import sympy as sp\n",
    "from sympy import exp, \\\n",
    "    cos, sin, tan, \\\n",
    "    cosh, sinh, tanh, ln\n",
    "\n",
    "x = sp.Symbol('x')\n",
    "\n",
    "# array manipulation\n",
    "import numpy as np\n",
    "import random as rn\n",
    "import math\n",
    "import time\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "\n",
    "# custom data prep and loader\n",
    "from seq2sequtil import loadData, \\\n",
    "Seq2SeqDataPreparer, Seq2SeqDataLoader\n",
    "\n",
    "# enable pretty printing of symbolic equations\n",
    "from IPython.display import display\n",
    "sp.init_printing(use_latex='mathjax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1626115818752,
     "user": {
      "displayName": "Harrison Prosper",
      "photoUrl": "",
      "userId": "06677011918885244894"
     },
     "user_tz": -120
    },
    "id": "EvmoUxfYuJdf"
   },
   "outputs": [],
   "source": [
    "#BASE = '/content/gdrive/My Drive/AI'\n",
    "BASE = '.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_rfrZXDE4l3n"
   },
   "source": [
    "Duration of an epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 385,
     "status": "ok",
     "timestamp": 1626115821978,
     "user": {
      "displayName": "Harrison Prosper",
      "photoUrl": "",
      "userId": "06677011918885244894"
     },
     "user_tz": -120
    },
    "id": "xOUoa7pT4l3n"
   },
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qjoD0FFjEHSc"
   },
   "source": [
    "Function to count the number of parameters in a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1626115823969,
     "user": {
      "displayName": "Harrison Prosper",
      "photoUrl": "",
      "userId": "06677011918885244894"
     },
     "user_tz": -120
    },
    "id": "riLbeY85EF5r"
   },
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gur9S9MWuJdi"
   },
   "source": [
    "### Load sequence data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2549,
     "status": "ok",
     "timestamp": 1626115831006,
     "user": {
      "displayName": "Harrison Prosper",
      "photoUrl": "",
      "userId": "06677011918885244894"
     },
     "user_tz": -120
    },
    "id": "TvB9KSRA4l3h",
    "outputId": "c49bfef7-e8aa-45c6-b5cb-2cb76506d0c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle - \\sinh{\\left(2 x \\right)}$"
      ],
      "text/plain": [
       "-sinh(2⋅x)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle - \\frac{4 x^{3}}{3} - 2 x$"
      ],
      "text/plain": [
       "     3      \n",
       "  4⋅x       \n",
       "- ──── - 2⋅x\n",
       "   3        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2000\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle e^{4 - 9 x^{2}} \\cos{\\left(7 x^{2} - 1 \\right)}$"
      ],
      "text/plain": [
       "        2              \n",
       " 4 - 9⋅x     ⎛   2    ⎞\n",
       "ℯ        ⋅cos⎝7⋅x  - 1⎠"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle x^{4} \\left(- 63 e^{4} \\sin{\\left(1 \\right)} + 16 e^{4} \\cos{\\left(1 \\right)}\\right) + x^{2} \\left(- 9 e^{4} \\cos{\\left(1 \\right)} + 7 e^{4} \\sin{\\left(1 \\right)}\\right) + e^{4} \\cos{\\left(1 \\right)}$"
      ],
      "text/plain": [
       " 4 ⎛      4              4       ⎞    2 ⎛     4             4       ⎞    4    \n",
       "x ⋅⎝- 63⋅ℯ ⋅sin(1) + 16⋅ℯ ⋅cos(1)⎠ + x ⋅⎝- 9⋅ℯ ⋅cos(1) + 7⋅ℯ ⋅sin(1)⎠ + ℯ ⋅cos\n",
       "\n",
       "   \n",
       "(1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4000\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left(- 2 x - 6\\right) e^{5 x - 2} + \\left(2 x^{3} - 7\\right) \\tanh{\\left(7 x^{2} - 5 \\right)}$"
      ],
      "text/plain": [
       "            5⋅x - 2   ⎛   3    ⎞     ⎛   2    ⎞\n",
       "(-2⋅x - 6)⋅ℯ        + ⎝2⋅x  - 7⎠⋅tanh⎝7⋅x  - 5⎠"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle x^{4} \\left(- 343 \\tanh{\\left(5 \\right)} - \\frac{2375}{12 e^{2}} + 343 \\tanh^{3}{\\left(5 \\right)}\\right) + x^{3} \\left(- \\frac{150}{e^{2}} - 2 \\tanh{\\left(5 \\right)}\\right) + x^{2} \\left(-49 - \\frac{85}{e^{2}} + 49 \\tanh^{2}{\\left(5 \\right)}\\right) - \\frac{32 x}{e^{2}} - \\frac{6}{e^{2}} + 7 \\tanh{\\left(5 \\right)}$"
      ],
      "text/plain": [
       "   ⎛                     -2               ⎞                                   \n",
       " 4 ⎜               2375⋅ℯ             3   ⎟    3 ⎛       -2            ⎞    2 \n",
       "x ⋅⎜-343⋅tanh(5) - ──────── + 343⋅tanh (5)⎟ + x ⋅⎝- 150⋅ℯ   - 2⋅tanh(5)⎠ + x ⋅\n",
       "   ⎝                  12                  ⎠                                   \n",
       "\n",
       "                                                           \n",
       "⎛          -2          2   ⎞         -2      -2            \n",
       "⎝-49 - 85⋅ℯ   + 49⋅tanh (5)⎠ - 32⋅x⋅ℯ   - 6⋅ℯ   + 7⋅tanh(5)\n",
       "                                                           "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6000\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\cos{\\left(\\frac{x}{3} \\right)} - \\tan{\\left(9 x - 3 \\right)}$"
      ],
      "text/plain": [
       "   ⎛x⎞               \n",
       "cos⎜─⎟ - tan(9⋅x - 3)\n",
       "   ⎝3⎠               "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle x^{4} \\left(4374 \\tan{\\left(3 \\right)} + 10935 \\tan^{3}{\\left(3 \\right)} + 6561 \\tan^{5}{\\left(3 \\right)} + 0.00051440329218107\\right) + x^{3} \\left(-243 - 972 \\tan^{2}{\\left(3 \\right)} - 729 \\tan^{4}{\\left(3 \\right)}\\right) + x^{2} \\left(81 \\tan{\\left(3 \\right)} + 81 \\tan^{3}{\\left(3 \\right)} - 0.0555555555555556\\right) + x \\left(-9 - 9 \\tan^{2}{\\left(3 \\right)}\\right) + \\tan{\\left(3 \\right)} + 1$"
      ],
      "text/plain": [
       " 4 ⎛                       3              5                         ⎞    3 ⎛  \n",
       "x ⋅⎝4374⋅tan(3) + 10935⋅tan (3) + 6561⋅tan (3) + 0.00051440329218107⎠ + x ⋅⎝-2\n",
       "\n",
       "            2             4   ⎞    2 ⎛                  3                     \n",
       "43 - 972⋅tan (3) - 729⋅tan (3)⎠ + x ⋅⎝81⋅tan(3) + 81⋅tan (3) - 0.0555555555555\n",
       "\n",
       "   ⎞     ⎛          2   ⎞             \n",
       "556⎠ + x⋅⎝-9 - 9⋅tan (3)⎠ + tan(3) + 1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "8000\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle e^{- 5 x - 1} \\cos{\\left(\\frac{x^{3}}{3} \\right)}$"
      ],
      "text/plain": [
       "             ⎛ 3⎞\n",
       " -5⋅x - 1    ⎜x ⎟\n",
       "ℯ        ⋅cos⎜──⎟\n",
       "             ⎝3 ⎠"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{625 x^{4}}{24 e} - \\frac{125 x^{3}}{6 e} + \\frac{25 x^{2}}{2 e} - \\frac{5 x}{e} + e^{-1}$"
      ],
      "text/plain": [
       "     4  -1        3  -1       2  -1                \n",
       "625⋅x ⋅ℯ     125⋅x ⋅ℯ     25⋅x ⋅ℯ          -1    -1\n",
       "────────── - ────────── + ───────── - 5⋅x⋅ℯ   + ℯ  \n",
       "    24           6            2                    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tcos(3*x**3/9)/exp(5*x+1)\n",
      "\n",
      "\texp(-1)-5*x*exp(-1)+25*x**2*exp(-1)/2-125*x**3*exp(-1)/6+625*x**4*exp(-1)/24\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs, targets = loadData('data/seq2seq_data_10000.txt')\n",
    "print(inputs[8000])\n",
    "print(targets[8000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert sequence data\n",
    "  1. Scan input and output sequences and construct maps of characters (tokens) to indices, one map for input sequences and another for target sequences.\n",
    "  1. Pad sequences to the same length\n",
    "  1. Split into train, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of seq-pairs (train):     8000\n",
      "number of seq-pairs (valid):     1000\n",
      "number of seq-pairs (test):      1000\n",
      "\n",
      "number of source tokens:           31\n",
      "max source sequence length:        81\n",
      "\n",
      "number of target tokens:           35\n",
      "max target sequence length:       883\n"
     ]
    }
   ],
   "source": [
    "fractions=[8/10, 9/10]\n",
    "db = Seq2SeqDataPreparer(inputs, targets, fractions)\n",
    "print(db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TjCtDW324l3i"
   },
   "source": [
    "## Implement encoder\n",
    "\n",
    "### LSTM \n",
    "\n",
    "An LSTM is a function that is typically conceptualized as a \"device\" containing various \"gates\" that filter input data in different ways. This creative conceptual reasoning has yielded functions with amazing capabilities. However, it is far from clear that this approach will be the way forward in the future. Why? Because we have compelling evidence that highly creative conceptual reasoning, while impressive, is not, actually, needed to arrive at functions with immense capability. The existence of human brains that have evolved through natural selection is an existence proof that immensely capable functions can be arrived at through trial and error.\n",
    "\n",
    "No doubt, one day, someone will devise an effective evolutionary algorithm that will compress millions of years of evolution into a matter of weeks or even days in order to construct immensely capable functions that could far outstrip what could be done through creative conceptual reasoning. \n",
    "\n",
    "#### LSTM Function\n",
    "\n",
    "At time step $t$, instances of the LSTM class in PyTorch compute the function\n",
    "\n",
    "\\begin{align*}\n",
    " i_t & = \\sigma(W_{ii} x_t + b_{ii} + W_{hi} h_{t-1} + b_{hi}),\\\\\n",
    " f_t & = \\sigma(W_{if} x_t + b_{if} + W_{hf} h_{t-1} + b_{hf}),\\\\\n",
    " g_t & = \\tanh(W_{ig} x_t + b_{ig} + W_{hg} h_{t-1} + b_{hg}),\\\\ \\\\\n",
    " c_t & = f_t \\odot c_{t-1} + i_t \\odot g_t,\\\\\n",
    " o_t & = \\sigma(W_{io} x_t + b_{io} + W_{ho} h_{t-1} + b_{ho}),\\\\\n",
    " h_t & = o_t \\odot \\tanh(c_t),\n",
    "\\end{align*}\n",
    "\n",
    "where $\\sigma$ is a sigmoid and $\\odot$ is the Hadamard product. The functions $i_t$, and $f_t$ are called the *input* and *forget* gates, respectively, while $c_t$ and $h_t$ are called the *cell* and *hidden* states, respectively, of the LSTM. The cell and hidden states $c_{t-1}$ and $h_{t-1}$ are from the previous time step. The number of elements that comprise the cell and hidden states is specified by the *hidden_size* argument of the PyTorch LSTM. The $W$s and $b$s are the parameters of the LSTM function. The LSTM outputs $o_t, (h_t, c_t)$.\n",
    "\n",
    "### PyTorch LSTM Parameters\n",
    "\n",
    "* input_size – The number of expected features in the input x\n",
    "* hidden_size – The number of features in the hidden state h\n",
    "* num_layers – Number of recurrent layers. E.g., setting num_layers=2 would mean stacking two LSTMs together to form a stacked LSTM, with the second LSTM taking in outputs of the first LSTM and computing the final results. Default: 1\n",
    "* bias – If False, then the layer does not use bias weights b_ih and b_hh. Default: True\n",
    "* batch_first – If True, then the input and output tensors are provided as (batch, seq, feature) instead of (seq, batch, feature). Note that this does not apply to hidden or cell states. See the Inputs/Outputs sections below for details. Default: False\n",
    "* dropout – If non-zero, introduces a Dropout layer on the outputs of each LSTM layer except the last layer, with dropout probability equal to dropout. Default: 0\n",
    "* bidirectional – If True, becomes a bidirectional LSTM. Default: False\n",
    "* proj_size – If > 0, will use LSTM with projections of corresponding size. Default: 0\n",
    "\n",
    "### Encoder\n",
    "  * max_seq_len - Maximum length of source, i.e. input, sequences\n",
    "  * num_features - Number of unique source tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 615,
     "status": "ok",
     "timestamp": 1626123088610,
     "user": {
      "displayName": "Harrison Prosper",
      "photoUrl": "",
      "userId": "06677011918885244894"
     },
     "user_tz": -120
    },
    "id": "JdD6YxTN4l3j"
   },
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 32\n",
    "NUM_LAYERS  = 10\n",
    "# use GPU if available\n",
    "DEVICE      = torch.device(\"cuda\" \\\n",
    "                           if torch.cuda.is_available() \\\n",
    "                           else \"cpu\")\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, max_seq_len, num_features, \n",
    "                 embed_size=10, \n",
    "                 hidden_size=HIDDEN_SIZE, \n",
    "                 num_layers=NUM_LAYERS, \n",
    "                 dropout=0.5,\n",
    "                 device=DEVICE):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.max_seq_len = max_seq_len  # unused at present\n",
    "        self.num_features= num_features\n",
    "        self.embed_size  = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers  = num_layers\n",
    "        self.dropout     = dropout\n",
    "        \n",
    "        # The embedding will map each of \"num_features\" indices into \n",
    "        # a vector of dimension embed_size, where typically, \n",
    "        # embed_size << num_features.\n",
    "        #\n",
    "        # The shape of the inputs to embedding is \n",
    "        #  (max_seq_len, batch_size).\n",
    "        #\n",
    "        # The shape of the outputs is \n",
    "        #  (max_seq_len, batch_size, embed_size)\n",
    "        self.embedding = nn.Embedding(num_features, \n",
    "                                      embed_size).to(device)\n",
    "        \n",
    "        # By default, LSTM expects the input shape to be\n",
    "        #  (max_seq_len, batch_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, \n",
    "                            hidden_size, \n",
    "                            num_layers,\n",
    "                            dropout=dropout).to(device)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x.shape: (max_seq_len, batch_size)\n",
    "       \n",
    "        x = self.embedding(x)\n",
    "        # x.shape: (max_seq_len, batch_size, embed_size)\n",
    "        \n",
    "        output, (hidden, cell) = self.lstm(x)\n",
    "        # output.shape: (max_seq_len, batch_size, hidden_size)\n",
    "\n",
    "        # we discard output of encoder\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q6SStaRM4l3j"
   },
   "source": [
    "## Implement Decoder\n",
    "\n",
    "The decoder is similar to the encoder in that it has an embedding layer that maps the target features (the indices associated with the characters in the target sequences) to vectors in a space of dimension that, ideally, is much less than the number of features, that is, the number of tokens.\n",
    "\n",
    "The key difference is that the output of the LSTM is passed to a *Linear* layer that outputs a vector equal in size to the number of features, i.e., tokens associated with the target sequences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 365,
     "status": "ok",
     "timestamp": 1626123091084,
     "user": {
      "displayName": "Harrison Prosper",
      "photoUrl": "",
      "userId": "06677011918885244894"
     },
     "user_tz": -120
    },
    "id": "uFyw9rHu4l3j"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    '''\n",
    "    max_seq_len  maximum length of target sequences\n",
    "    num_features number of target features, i.e., tokens\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 max_seq_len,\n",
    "                 num_features, \n",
    "                 embed_size=10, \n",
    "                 hidden_size=HIDDEN_SIZE, \n",
    "                 num_layers=NUM_LAYERS, \n",
    "                 dropout=0.5, \n",
    "                 device=DEVICE):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.max_seq_len = max_seq_len   # this is used below\n",
    "        self.num_features= num_features  # number of target tokens\n",
    "        self.embed_size  = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers  = num_layers\n",
    "        self.dropout     = dropout\n",
    "        self.device      = device\n",
    " \n",
    "        # The shape of inputs must be (max_seq_len, batch_size)\n",
    "        # The shape of outputs is (max_seq_len, batch_size, embed_size)\n",
    "        self.embedding   = nn.Embedding(num_features, \n",
    "                                        embed_size).to(device)\n",
    "\n",
    "        # inputs.shape:  (max_seq_len, batch_size, embed_size)\n",
    "        # outputs.shape: (max_seq_len, batch_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(input_size  = embed_size, \n",
    "                            hidden_size = hidden_size, \n",
    "                            num_layers  = num_layers,\n",
    "                            dropout     = dropout).to(device)\n",
    "\n",
    "        self.linear = nn.Linear(hidden_size, num_features).to(device)\n",
    "\n",
    "    def forward(self, source, hidden, cell):\n",
    "        # For a given batch instance, the indices of tokens will be \n",
    "        # passed one by one to this function. Therefore, the source\n",
    "        # is a 1D tensor of token indices of shape (batch_size).\n",
    "        # But since the embedding object requires an input with shape \n",
    "        # (max_seq_len, batch_size) we need to use unsqueeze(0) to \n",
    "        # insert an extra dimension of size 1 at dim=0 so that the\n",
    "        # shape is (1, batch_size).\n",
    "        \n",
    "        source = source.unsqueeze(0).to(self.device)\n",
    "\n",
    "        # source.shape:   (1, batch_size)\n",
    "        \n",
    "        # Map each index in the batch of indices to its embedded \n",
    "        # representation.\n",
    "        embedded = self.embedding(source)\n",
    "        # embedded.shape: (1, batch_size, embed_size)\n",
    "                \n",
    "        # Input embedded representation of token and previous\n",
    "        # hidden and cell states to lstm\n",
    "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "        # output.shape:   (1, batch_size, hidden_size)\n",
    "        \n",
    "        # Get rid of dim 0 of length \"1\" to match shape expected by\n",
    "        # the linear layer\n",
    "        output = output.squeeze(0) \n",
    "        # output.shape:   (batch_size, hidden_size)\n",
    "        \n",
    "        prediction = self.linear(output)\n",
    "        # prediction.shape: (batch_size, num_features)\n",
    "        \n",
    "        # We don't need to apply a softmax to prediction because \n",
    "        # this is done by the PyTorch cross entropy class. \n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jhwo7q1Z4l3k"
   },
   "source": [
    "### Construct seq2seq model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1626123093159,
     "user": {
      "displayName": "Harrison Prosper",
      "photoUrl": "",
      "userId": "06677011918885244894"
     },
     "user_tz": -120
    },
    "id": "Q35uffAu4l3k"
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    '''\n",
    "    model = Model(encoder, decoder, device)\n",
    "    '''\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device  = device\n",
    "        self.max_seq_len  = self.decoder.max_seq_len\n",
    "        self.num_features = self.decoder.num_features\n",
    "        \n",
    "        assert encoder.hidden_size == decoder.hidden_size, \\\n",
    "            \"hidden_size of encoder and decoder must be equal!\"\n",
    "        \n",
    "        assert encoder.num_layers == decoder.num_layers, \\\n",
    "            \"num_layers of encoder and decoder must be equal!\"\n",
    "        \n",
    "    def forward(self, source, target=None):\n",
    "        '''\n",
    "    1.  If target is a tuple, then it is interpreted as:\n",
    "        target = (target, teacher_prob)\n",
    "            \n",
    "    2.  If the target is not a tuple, it is assumed to be the target\n",
    "        and the teacher_prob is set to 0.5\n",
    "        \n",
    "    When the model is called in eval mode, only the source need be\n",
    "    given, in which case teacher_prob is set to 0.0.\n",
    "        '''\n",
    "\n",
    "        source = source.to(self.device)\n",
    "        if target == None:\n",
    "            # the target is not used in eval mode\n",
    "            teacher_prob = 0.0\n",
    "        elif type(target) == type(()):\n",
    "            target, teacher_prob = target\n",
    "            target = target.to(self.device)\n",
    "        else:\n",
    "            teacher_prob = 0.5\n",
    "            target = target.to(self.device)\n",
    "\n",
    "        # (see definition of x_max_seq_len in Seq2SeqDataPreparer)\n",
    "        # source.shape = (x_max_seq_len, batch_size)\n",
    "        # target.shape = (y_max_seq_len, batch_size)\n",
    "        #\n",
    "        # teacher_prob is the probability, during training, to \n",
    "        # use the true target rather than the predicted target.\n",
    "        # ideally that probability should be gradually reduced as the\n",
    "        # training progresses.\n",
    "           \n",
    "        # Tensor to store decoder outputs\n",
    "        batch_size = source.shape[1] # same for targets\n",
    "        outputs = torch.zeros(self.max_seq_len, \n",
    "                              batch_size, \n",
    "                              self.num_features).to(self.device)\n",
    "        \n",
    "        # Use last hidden state of the encoder as the initial \n",
    "        # hidden state of the decoder. Note: the encoder discards\n",
    "        # the output of the LSTM.\n",
    "        hidden, cell = self.encoder(source)\n",
    "        \n",
    "        # Loop over the source indices corresponding to tokens.\n",
    "        # The first input to the decoder should be the index \n",
    "        # associated with the tab token. \n",
    "        index = source[0,:]\n",
    "        \n",
    "        for t in range(1, self.max_seq_len):\n",
    "            \n",
    "            output, hidden, cell = self.decoder(index, hidden, cell)\n",
    "            \n",
    "            # Cache predictions for each token\n",
    "            # output.shape: (batch_size, num_features)\n",
    "            outputs[t] = output\n",
    "                       \n",
    "            # Get index of prediction with highest value\n",
    "            prediction = output.argmax(1) \n",
    "            \n",
    "            # For the next index, decide whether to use the\n",
    "            # target or the prediction. Note: in eval mode, since\n",
    "            # only the source is specified, teacher_prob will be \n",
    "            # zero (by construction) so that the predicted indices,\n",
    "            # and therefore tokens, will be used.\n",
    "            use_target = rn.random() < teacher_prob\n",
    "            index      = target[t] if use_target else prediction\n",
    "        \n",
    "        # Note: For a given batch instance, the num_features outputs\n",
    "        # do not sum to unity.\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1yD5JTRC4l3k"
   },
   "source": [
    "### Train Model\n",
    "\n",
    "  1. Create an encoder\n",
    "  1. Create a decoder\n",
    "  1. Create a encoder/decoder model\n",
    "  1. Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1626123094274,
     "user": {
      "displayName": "Harrison Prosper",
      "photoUrl": "",
      "userId": "06677011918885244894"
     },
     "user_tz": -120
    },
    "id": "LZ0sDFXH4l3l",
    "outputId": "171f84ed-cd48-4a28-fa8a-a13b391d97f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(31, 5)\n",
      "    (lstm): LSTM(5, 32, num_layers=10, dropout=0.5)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (embedding): Embedding(35, 5)\n",
      "    (lstm): LSTM(5, 32, num_layers=10, dropout=0.5)\n",
      "    (linear): Linear(in_features=32, out_features=35, bias=True)\n",
      "  )\n",
      ")\n",
      "Computational device:      cpu\n",
      "Number of free parameters: 163533\n"
     ]
    }
   ],
   "source": [
    "max_source_seq_len  = db.max_seq_len('source')\n",
    "num_source_features = db.num_tokens('source')\n",
    "encoder_embed_size  = 5\n",
    "encoder_dropout     = 0.5\n",
    "\n",
    "max_target_seq_len  = db.max_seq_len('target')\n",
    "num_target_features = db.num_tokens('target')\n",
    "decoder_embed_size  = 5\n",
    "decoder_dropout     = 0.5\n",
    "\n",
    "# in this model, the following structural parameters are the same for\n",
    "# both the encoder and decoder. \n",
    "hidden_size         = HIDDEN_SIZE  # size of hidden and cell state vectors\n",
    "num_layers          = NUM_LAYERS   # number of stacked LSTMs\n",
    "\n",
    "encoder = Encoder(max_source_seq_len,\n",
    "                  num_source_features, \n",
    "                  encoder_embed_size, \n",
    "                  hidden_size, \n",
    "                  num_layers, \n",
    "                  encoder_dropout).to(DEVICE)\n",
    "\n",
    "decoder = Decoder(max_target_seq_len,\n",
    "                  num_target_features, \n",
    "                  decoder_embed_size, \n",
    "                  hidden_size, \n",
    "                  num_layers, \n",
    "                  decoder_dropout).to(DEVICE)\n",
    "\n",
    "model = Model(encoder, decoder, DEVICE).to(DEVICE)\n",
    "print(model)\n",
    "print('Computational device:      %s' % DEVICE)\n",
    "print('Number of free parameters: %d' % count_parameters(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nv6qIvR34l3l"
   },
   "source": [
    "### Choose optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 565,
     "status": "ok",
     "timestamp": 1626123106056,
     "user": {
      "displayName": "Harrison Prosper",
      "photoUrl": "",
      "userId": "06677011918885244894"
     },
     "user_tz": -120
    },
    "id": "d1eEV5Am4l3l"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8FJGemwi4l3l"
   },
   "source": [
    "### Choose loss function\n",
    "For each token with associated index k, compute the cross-entropy loss\n",
    "\n",
    "\\begin{align*}\n",
    "    E_k & = -\\log \\left( \\frac{\\exp(\\hat{y}_k)}{\\sum_{\\{j\\}} \\exp(\\hat{y}_j) } \\right) ,\n",
    "\\end{align*}\n",
    "\n",
    "where $\\hat{y}_j$ is the $j^\\text{th}$ element of the model's output vector of length *num_features*. The losses $E_k$ are averaged over all tokens in a sequence and all batch instances. The set $\\{ j \\}$  excludes tokens that correspond to padding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1626123108051,
     "user": {
      "displayName": "Harrison Prosper",
      "photoUrl": "",
      "userId": "06677011918885244894"
     },
     "user_tz": -120
    },
    "id": "C0HLgdIl4l3l"
   },
   "outputs": [],
   "source": [
    "space = db.x_token2index[' ']\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=space).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TXchIB4x4l3l"
   },
   "source": [
    "### Define trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1626123109706,
     "user": {
      "displayName": "Harrison Prosper",
      "photoUrl": "",
      "userId": "06677011918885244894"
     },
     "user_tz": -120
    },
    "id": "Uu0qrPWX4l3m"
   },
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, criterion, device, clip):\n",
    "    \n",
    "    # set to train mode\n",
    "    model.train()  \n",
    "    \n",
    "    epoch_loss  = 0\n",
    "    count = 0\n",
    "    \n",
    "    # Important: reset dataloader to recreate the iterator.\n",
    "    # This is necessary because Python's iter(*) class has no means\n",
    "    # to reset it.\n",
    "    dataloader.reset() \n",
    "    \n",
    "    for i, (X, Y) in enumerate(dataloader):\n",
    "        #print('X.shape:', X.shape, ' Y.shape:', Y.shape)\n",
    "        \n",
    "        # zero gradients of all trainable parameters\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # the dataloader has been initialized with pinned memory so\n",
    "        # that, in principle, the transfer to the device is faster.\n",
    "        X = X.to(device)\n",
    "        Y = Y.to(device)\n",
    "        \n",
    "        # compute output of model\n",
    "        output = model(X, Y)\n",
    "        # output.shape: (max_target_seq_len, batch_size, num_features)\n",
    "        #print('output.shape(before):', output.shape)\n",
    "    \n",
    "        num_features = output.shape[-1]\n",
    "        \n",
    "        # skip first token (a tab), then reshape to\n",
    "        # ((max_target_seq_len-1)*batch_size, num_features)\n",
    "        output = output[1:].reshape((-1, num_features))\n",
    "        #print('output.shape(after):', output.shape)\n",
    "\n",
    "        # skip first token (a tab), then reshape to\n",
    "        # ((max_target_seq_len-1)*batch_size)\n",
    "        Y = Y[1:].reshape(-1)\n",
    "               \n",
    "        # average loss over mini-batches\n",
    "        loss = criterion(output, Y)\n",
    "        \n",
    "        # compute gradients using automatic differentiation\n",
    "        loss.backward()\n",
    "        \n",
    "        # clip gradients so they don't blow up\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        # make one step in th model parameter space\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item() # to CPU, if using a GPU\n",
    "        count += 1\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            print('%d ' % i, end='')\n",
    "            \n",
    "    print('%d end epoch' % count)\n",
    "    return epoch_loss / count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "voIKn--S4l3m"
   },
   "source": [
    "### Evaluator\n",
    "\n",
    "In evaluation mode, we turn off gradient calculation and we provide only the source sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1626123111655,
     "user": {
      "displayName": "Harrison Prosper",
      "photoUrl": "",
      "userId": "06677011918885244894"
     },
     "user_tz": -120
    },
    "id": "03jnoaBr4l3m"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, criterion, device):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    count = 0\n",
    "    \n",
    "    # no need to compute gradients\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        # Important: reset dataloader to recreate iterator\n",
    "        dataloader.reset()\n",
    "\n",
    "        for i, (X, Y) in enumerate(dataloader):\n",
    "            X = X.to(device)\n",
    "            Y = Y.to(device)\n",
    "\n",
    "            output = model(X)\n",
    "            # output.shape:(max_target_seq_len,batch_size,num_features)\n",
    "\n",
    "            num_features = output.shape[-1]\n",
    "        \n",
    "            # skip first token (a tab), then reshape to\n",
    "            # ((max_target_seq_len-1)*batch_size, num_features)\n",
    "            output = output[1:].reshape((-1, num_features))\n",
    "                            \n",
    "            # skip first token (a tab), then reshape targets to\n",
    "            # ((max_target_seq_len-1)*batch_size)\n",
    "            Y = Y[1:].reshape(-1)\n",
    "               \n",
    "            # average loss over mini-batches, excluding padding tokens\n",
    "            loss = criterion(output, Y)\n",
    "        \n",
    "            epoch_loss += loss.item()\n",
    "            count += 1\n",
    "            \n",
    "    return epoch_loss / count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "47irOA8H4l3n"
   },
   "source": [
    "### Finally, train!\n",
    "\n",
    "\n",
    "Note: The class __Seq2SeqDataLoader__ is a custom dataloader that returns batches of sequence pairs comprising sources and targets, X and Y, respectively. The quantities X and Y are 2D tensors, each with shape (max_seq_len, batch_size) that comprise sequences of indices arranged in columns. Each index corresponds to a token, i.e., a character. The column lengths, max_seq_len, differ between X and Y.\n",
    "\n",
    "\n",
    "__NB:__ At present, Python iterators can't be reset, so we need to remake them each time to ensure each starts at the beginning. We do so by calling the __reset()__ method of __Seq2SeqDataLoader__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "executionInfo": {
     "elapsed": 494,
     "status": "ok",
     "timestamp": 1626123114625,
     "user": {
      "displayName": "Harrison Prosper",
      "photoUrl": "",
      "userId": "06677011918885244894"
     },
     "user_tz": -120
    },
    "id": "al20iuoOJljr"
   },
   "outputs": [],
   "source": [
    "#=================================================================\n",
    "BATCH_SIZE = 50      # number of instances per batch\n",
    "N_EPOCHS   = 10      # number of times through training data\n",
    "CLIP       = 1       # prevent gradients from exploding!\n",
    "#=================================================================\n",
    "train_dataloader = Seq2SeqDataLoader(db.x_train, db.y_train, BATCH_SIZE)\n",
    "valid_dataloader = Seq2SeqDataLoader(db.x_valid, db.y_valid, BATCH_SIZE)\n",
    "test_dataloader  = Seq2SeqDataLoader(db.x_test,  db.y_test,  BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ruLv1WxG4l3n",
    "outputId": "e0e288b5-6237-4e84-d48d-cf5021e09217"
   },
   "outputs": [],
   "source": [
    "def train_me():\n",
    "    best_valid_loss = float('inf')\n",
    "\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        print('epoch: %5d' % epoch)\n",
    "    \n",
    "        start_time = time.time()\n",
    "    \n",
    "        train_loss = train(model, train_dataloader, optimizer, \n",
    "                           criterion, DEVICE, CLIP)\n",
    "    \n",
    "        valid_loss = evaluate(model, valid_dataloader, \n",
    "                              criterion, DEVICE)\n",
    "    \n",
    "        end_time   = time.time()\n",
    "        mins, secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "        if valid_loss < best_valid_loss:\n",
    "            # save best model so far\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(),\n",
    "                       '%s/seq2seq_model.pt' % BASE)\n",
    "    \n",
    "        print(' duration(mm:ss): %2.2d:%2.2d | train_loss: %7.4f |'\\\n",
    "              ' valid_loss: %7.4f\\n' % \\\n",
    "              (mins, secs, train_loss, valid_loss))\n",
    "    print('\\ndone!')\n",
    "    \n",
    "train_me()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uo6Wp4Wk4l3n"
   },
   "source": [
    "### Now test!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 244
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "error",
     "timestamp": 1626105849454,
     "user": {
      "displayName": "Harrison Prosper",
      "photoUrl": "",
      "userId": "06677011918885244894"
     },
     "user_tz": -120
    },
    "id": "ZAelscAw4l3n",
    "outputId": "bc4cd2f1-d297-45eb-b53e-9d98324814ae"
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('%s/seq2seq_model.pt' % BASE))\n",
    "test_loss = evaluate(model, test_dataloader, criterion, DEVICE)\n",
    "print('test loss: %7.4f' % test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yCOVvH8eGkl2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "seq2seq_train.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
