{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fELG9vDJuJdc"
   },
   "source": [
    "# seq2seq: Data Preparation\n",
    "\n",
    "Abdulhakim Alnuqaydan, Ali Kadhim, Sergei Gleyzer, Harrison Prosper\n",
    "\n",
    "July 2021\n",
    "\n",
    "This notebook performs the following tasks:\n",
    "  1. Read the sequence pairs from __data/seq2seq_data.txt__.\n",
    "  1. Exclude sequences with complex numbers and with Taylor series expansions longer than 1000 characters.\n",
    "  1. Write the filtered sequences to __data/seq2seq_data_count.txt__, where count is either 10,000 or 60,000 sequences.\n",
    "  1. Read filtered data and delimit source (i.e, input) and target (i.e., output) sequences with a tab and newline at the start and end of each sequence, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "GwFXx5YluJde"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import sympy as sp\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# symbolic symbols\n",
    "from sympy import exp, \\\n",
    "    cos, sin, tan, \\\n",
    "    cosh, sinh, tanh, ln\n",
    "x = sp.Symbol('x')\n",
    "\n",
    "from IPython.display import display\n",
    "    \n",
    "# enable pretty printing of equations\n",
    "sp.init_printing(use_latex='mathjax')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output file: data/seq2seq_data_10000.txt\n",
      "output file: data/seq2seq_data_60000.txt\n"
     ]
    }
   ],
   "source": [
    "of_order = re.compile(' [+] O[(]x[*][*]5.*[)]')\n",
    "add_count= re.compile('_data')\n",
    "def filterData(inpfile='data/seq2seq_data.txt',\n",
    "               num_seq=60000, # number of sequences\n",
    "               max_len=260): # maximum length of target sequences\n",
    "    \n",
    "    # eliminate instances involving complex numbers\n",
    "    data = filter(lambda d: d.find('I') < 0, open(inpfile).readlines())\n",
    "    data = list(data)\n",
    " \n",
    "    # keep expansions that are less than maxlen characters long\n",
    "    data = filter(lambda d: len(d) < max_len, data)\n",
    "    data = list(data)\n",
    "    \n",
    "    # strip away O(...), that is, of order..\n",
    "    data = [of_order.sub('', d) for d in data]\n",
    "\n",
    "    N = min(num_seq, len(data))\n",
    "    outfile = add_count.sub('_data_%d' % N, inpfile)\n",
    "    print('output file:', outfile)\n",
    "    open(outfile, 'w').writelines(data[:N])\n",
    "    \n",
    "filterData(num_seq=10000, max_len=260)\n",
    "filterData(num_seq=60000, max_len=260)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map sequences to lists of indices\n",
    "\n",
    "  1. Split data into a train, validation, and test set.\n",
    "  1. Create a token (i.e., a character) to index map from training data.\n",
    "  1. Map sequences to arrays of indices.\n",
    "  1. Implement custom DataLoader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting seq2sequtil.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile seq2sequtil.py\n",
    "import numpy as np\n",
    "import torch\n",
    "from IPython.display import display\n",
    "\n",
    "# symbolic symbols\n",
    "from sympy import Symbol, exp, \\\n",
    "    cos, sin, tan, \\\n",
    "    cosh, sinh, tanh, ln\n",
    "x = Symbol('x')\n",
    "\n",
    "class Seq2SeqDataPreparer:\n",
    "    '''\n",
    "    This class maps the source (i.e., input) and target (i.e, output) \n",
    "    sequences of characters into sequences of indices. The source data \n",
    "    are split into x_train, x_valid, and x_test sets and similarly for \n",
    "    the target data.\n",
    "    \n",
    "    Create a data preparer using\n",
    "    \n",
    "    dd = Seq2SeqDataPreparer(X, Y, fractions)\n",
    "    \n",
    "    where,\n",
    "\n",
    "      fractions:    a 2-tuple containing the three-way split of data.\n",
    "                    e.g.: (5/6, 5.5/6) means split the data as follows\n",
    "                    (50000, 5000, 5000)\n",
    "    '''\n",
    "    def __init__(self, X, Y,\n",
    "                 fractions=[5/6,5.5/6], \n",
    "                 max_batch_size=64):\n",
    "        \n",
    "        self.fractions = fractions\n",
    "        self.max_batch_size = max_batch_size\n",
    "        \n",
    "        # get maximum sequence length for input expressions\n",
    "        self.x_max_seq_len =  max([len(z) for z in X])\n",
    "        \n",
    "        # get maximum sequence length for target expressions\n",
    "        self.y_max_seq_len =  max([len(z) for z in Y])\n",
    "        \n",
    "        # get length of splits into train, valid, test\n",
    "        N = int(len(X)*fractions[0])\n",
    "        M = int(len(X)*fractions[1])\n",
    "        \n",
    "        # create token to index map for source sequences\n",
    "        t = self.token_tofrom_index(X[:N])\n",
    "        self.x_token2index, self.x_index2token = t\n",
    "        \n",
    "        # create token to index map for target sequences\n",
    "        t = self.token_tofrom_index(Y[:N])\n",
    "        self.y_token2index,self.y_index2token = t\n",
    "        \n",
    "        # structure data into a list of blocks, where each block\n",
    "        # comprises a tuple (x_data, y_data) whose elements have\n",
    "        #   x_data.shape: (x_max_seq_len, block_size)\n",
    "        #   y_data.shape: (y_max_seq_len, block_size)\n",
    "        # and max_batch_size < block_size.\n",
    "        self.train_data,self.n_train= self.code_data(X[:N], Y[:N])         \n",
    "        self.valid_data,self.n_valid= self.code_data(X[N:M],Y[N:M])\n",
    "        self.test_data,self.n_test  = self.code_data(X[M:], Y[M:])\n",
    "\n",
    "    def __del__(self):\n",
    "        pass\n",
    "    \n",
    "    def __len__(self):\n",
    "        n  = 0\n",
    "        n += self.n_train\n",
    "        n += self.n_valid\n",
    "        n += self.n_test\n",
    "        return n\n",
    "    \n",
    "    def __str__(self):\n",
    "        s  = ''\n",
    "        s += 'number of seq-pairs (train): %8d\\n' % self.n_train\n",
    "        s += 'number of seq-pairs (valid): %8d\\n' % self.n_valid\n",
    "        s += 'number of seq-pairs (test):  %8d\\n' % self.n_test\n",
    "        s += '\\n'\n",
    "        s += 'number of source tokens:     %8d\\n' % \\\n",
    "        len(self.x_token2index)\n",
    "        s += 'max source sequence length:  %8d\\n' % \\\n",
    "        self.x_max_seq_len\n",
    "        \n",
    "        try:\n",
    "            s += '\\n'\n",
    "            s += 'number of target tokens:     %8d\\n' % \\\n",
    "            len(self.y_token2index)\n",
    "            s += 'max target sequence length:  %8d' % \\\n",
    "            self.y_max_seq_len\n",
    "        except:\n",
    "            pass\n",
    "        return s\n",
    "         \n",
    "    def num_tokens(self, which='source'):\n",
    "        if which[0] in ['s', 'i']:\n",
    "            return len(self.x_token2index)\n",
    "        else:\n",
    "            return len(self.y_token2index)\n",
    "    \n",
    "    def max_seq_len(self, which='source'):\n",
    "        if which[0] in ['s', 'i']:\n",
    "            return self.x_max_seq_len\n",
    "        else:\n",
    "            return self.y_max_seq_len\n",
    "        \n",
    "    def decode(self, indices):\n",
    "        # map list of indices to a list of tokens\n",
    "        return ''.join([self.y_index2token[i] for i in indices])\n",
    "\n",
    "    def token_tofrom_index(self, expressions):\n",
    "        chars = set()\n",
    "        chars.add(' ')  # for padding\n",
    "        chars.add('?')  # for unknown characters\n",
    "        for expression in expressions:\n",
    "            for char in expression:\n",
    "                chars.add(char)\n",
    "        chars = sorted(list(chars))\n",
    "        \n",
    "        char2index = dict([(char, i) for i, char in enumerate(chars)])\n",
    "        index2char = dict([(i, char) for i, char in enumerate(chars)])\n",
    "        return (char2index, index2char)\n",
    "        \n",
    "    def code_data(self, X, Y):\n",
    "        # X, Y consist of delimited strings: \n",
    "        #   \\tab<characters\\newline\n",
    "        \n",
    "        # loop over sequence pairs and convert them to sequences\n",
    "        # of integers using the two token2index maps\n",
    "      \n",
    "        x_space   = self.x_token2index[' ']\n",
    "        x_unknown = self.x_token2index['?']\n",
    "        \n",
    "        y_space   = self.y_token2index[' ']\n",
    "        y_unknown = self.y_token2index['?']\n",
    "        \n",
    "        cdata     = []  \n",
    "        for i, (x_expression, y_expression) in enumerate(zip(X, Y)):\n",
    "            \n",
    "            # ------------------------------------------\n",
    "            # map source characters to integers\n",
    "            # ------------------------------------------\n",
    "            x_n = len(x_expression)\n",
    "            source = [0] * x_n \n",
    "            for t, char in enumerate(x_expression):\n",
    "                try:\n",
    "                    source[t] = self.x_token2index[char]\n",
    "                except:\n",
    "                    source[t] = x_unknown\n",
    "            \n",
    "            # ------------------------------------------\n",
    "            # map target characters to integers\n",
    "            # ------------------------------------------\n",
    "            y_n = len(y_expression)\n",
    "            target = [0] * y_n \n",
    "            for t, char in enumerate(y_expression):\n",
    "                try:\n",
    "                    target[t] = self.y_token2index[char]\n",
    "                except:\n",
    "                    target[t] = y_unknown\n",
    "                    \n",
    "            # Structure data as a list of 4-tuples, with the first\n",
    "            # element of the tuple the length of the target sequence,\n",
    "            # which, in this example, tend to be longer than the\n",
    "            # source sequences. We'll sort the 4-tuples into\n",
    "            # ascending order of target sequence length.\n",
    "            cdata.append((y_n, x_n, target, source))\n",
    "         \n",
    "        # ---------------------------------------------------------\n",
    "        # Group data according to length of target sequence\n",
    "        # ---------------------------------------------------------\n",
    "        # 1. Sort sequence pairs according to target sequence lengths    \n",
    "        cdata.sort() \n",
    "        \n",
    "        # 2. Compute number of blocks of data, n_blocks, each of \n",
    "        #    whose target sequences will have roughly the same \n",
    "        #    sequence length.\n",
    " \n",
    "        n_data   = len(cdata)            # number of sequence pairs\n",
    "        l_block  = self.max_batch_size + 16   # length of blocks\n",
    "        n_blocks = int(n_data / l_block) # number of blocks\n",
    "        \n",
    "        # Note: the last block will, in general, have a length >= \n",
    "        #       to the length of the other blocks.\n",
    "        \n",
    "        # 3. Loop over blocks and and pad sequences so that all\n",
    "        #    sequences within a block are of the same length. \n",
    "        #    Do this separately for source and target sequences. \n",
    "        #    The shape of each block is (max_seq_len, block-size), \n",
    "        #    where max_seq_len can change from block to block.\n",
    "    \n",
    "        blocks = [0] * n_blocks\n",
    "        for k in range(n_blocks):\n",
    "            \n",
    "            # get block k\n",
    "            \n",
    "            i = k * l_block              # start of block k\n",
    "            if k < n_blocks - 1:\n",
    "                j = i + l_block - 1      # end of block k\n",
    "            else:\n",
    "                j = n_data - 1\n",
    "\n",
    "            block = cdata[i:j]\n",
    " \n",
    "            # get minimum and maximum length of target sequences\n",
    "            # for current block\n",
    "            y_min_seq_len, _, _, _ = block[0]\n",
    "            y_max_seq_len, _, _, _ = block[-1]\n",
    "        \n",
    "            # for current block source sequences need not be \n",
    "            # ordered so get maximum of all \n",
    "            # source sequences for current block.\n",
    "            x_min_seq_len = min([x_len for _, x_len, _, _ in block])\n",
    "            x_max_seq_len = max([x_len for _, x_len, _, _ in block])\n",
    "   \n",
    "            # loop over sequence pairs in current block\n",
    "            # and pad them to the same length, separately for the\n",
    "            # source and target sequences\n",
    "           \n",
    "            # create empty arrays for sequences\n",
    "            block_len = len(block)\n",
    "            \n",
    "            x_data = np.zeros((x_max_seq_len, block_len), dtype='long')\n",
    "            y_data = np.zeros((y_max_seq_len, block_len), dtype='long')\n",
    "    \n",
    "            for j, (y_seq_len, x_seq_len, \n",
    "                    y_seq, x_seq) in enumerate(block):\n",
    "            \n",
    "                # copy source data to 2D arrays\n",
    "                for t, c in enumerate(x_seq): \n",
    "                    x_data[t, j] = c\n",
    "                    \n",
    "                # pad source data\n",
    "                if x_seq_len < x_max_seq_len: \n",
    "                    x_data[t + 1:, j] = x_space\n",
    "                \n",
    "                # copy target data to 2D arrays\n",
    "                for t, c in enumerate(y_seq): \n",
    "                    y_data[t, j] = c\n",
    "                    \n",
    "                # pad array data\n",
    "                if y_seq_len < y_max_seq_len: \n",
    "                    y_data[t + 1:, j] = y_space\n",
    "            \n",
    "            # cache padded data\n",
    "            blocks[k] = (x_data, y_data)\n",
    "            \n",
    "            #print('%5d\\t%5d %5d\\t%5d %5d' % \\\n",
    "            #      (k, \n",
    "            #       x_min_seq_len, x_max_seq_len,\n",
    "            #       y_min_seq_len, y_max_seq_len))\n",
    "\n",
    "        return blocks, n_data\n",
    "    \n",
    "class Seq2SeqDataLoader:\n",
    "    '''\n",
    "    dataloader = Seq2seqDataLoader(dataset, device, batch_size=64)    \n",
    "    '''\n",
    "    def __init__(self, dataset, device,\n",
    "                 batch_size=64): \n",
    "        self.dataset    = dataset\n",
    "        self.device     = device\n",
    "        self.batch_size = batch_size\n",
    "        self.count      = 0\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        self.count += 1\n",
    "        if self.count <= len(self.dataset):\n",
    "            # 1. randomly pick a block\n",
    "            k  = np.random.randint(len(self.dataset))\n",
    "            \n",
    "            # 2. get its block_size > batch_size\n",
    "            block_size = self.dataset[k][0].shape[-1]\n",
    "            assert block_size > self.batch_size\n",
    "            \n",
    "            # 3. randomly pick a unique set of \"batch_size\"\n",
    "            #    integers from the array of integers jj\n",
    "            jj = np.arange(block_size)\n",
    "            ii = np.random.choice(jj, self.batch_size, replace=False)\n",
    "            \n",
    "            # 4. create tensors directly on the device of interest\n",
    "            X = torch.tensor(self.dataset[k][0][:, ii], \n",
    "                             device=self.device)\n",
    "            \n",
    "            Y = torch.tensor(self.dataset[k][1][:, ii], \n",
    "                             device=self.device)\n",
    "        \n",
    "            # shape of X and Y: (max_seq_len, batch_size)\n",
    "            return X, Y\n",
    "        else:\n",
    "            self.count = 0\n",
    "            raise StopIteration\n",
    "        \n",
    "# Delimit each sequence in filtered sequences\n",
    "# The start of sequence (SOS) and end of sequence (EOS) \n",
    "# tokens are \"\\t\" and \"\\n\", respectively.\n",
    "\n",
    "def loadData(inpfile):\n",
    "    # format of data:\n",
    "    # input expression<tab>target expression<newline>\n",
    "    data = [a.split('\\t') for a in open(inpfile).readlines()]\n",
    "    \n",
    "    X, Y = [], []\n",
    "    for i, (x, y) in enumerate(data):\n",
    "        X.append('\\t%s\\n' % x)\n",
    "        # get rid of spaces in target sequence\n",
    "        y = ''.join(y.split())\n",
    "        Y.append('\\t%s\\n' % y)\n",
    "        \n",
    "    print('Example source:')\n",
    "    print(X[-1])\n",
    "    pprint(X[-1])\n",
    "    print('Example target:')\n",
    "    print(Y[-1])\n",
    "    pprint(Y[-1])\n",
    "\n",
    "    return (X, Y)\n",
    "\n",
    "def pprint(expr):\n",
    "    display(eval(expr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display a few sequence pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example source:\n",
      "\t-(9*x/3)*tanh(-2*x**3+9)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle - 3 x \\tanh{\\left(9 - 2 x^{3} \\right)}$"
      ],
      "text/plain": [
       "         ⎛       3⎞\n",
       "-3⋅x⋅tanh⎝9 - 2⋅x ⎠"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example target:\n",
      "\t-3*x*tanh(9)+x**4*(6-6*tanh(9)**2)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle x^{4} \\left(6 - 6 \\tanh^{2}{\\left(9 \\right)}\\right) - 3 x \\tanh{\\left(9 \\right)}$"
      ],
      "text/plain": [
       " 4 ⎛          2   ⎞              \n",
       "x ⋅⎝6 - 6⋅tanh (9)⎠ - 3⋅x⋅tanh(9)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t(9*x**3+6)*cos(-8*x**3/7)/sin(2*x**3-3)\n",
      "\n",
      "\t-6/sin(3)+x**3*(-9/sin(3)-12*cos(3)/sin(3)**2)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import seq2sequtil as sq\n",
    "import importlib\n",
    "importlib.reload(sq)\n",
    "inputs, targets = sq.loadData('data/seq2seq_data_10000.txt')\n",
    "print(inputs[8000])\n",
    "print(targets[8000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check data preparer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of seq-pairs (train):     8000\n",
      "number of seq-pairs (valid):     1000\n",
      "number of seq-pairs (test):      1000\n",
      "\n",
      "number of source tokens:           31\n",
      "max source sequence length:        81\n",
      "\n",
      "number of target tokens:           35\n",
      "max target sequence length:       214\n"
     ]
    }
   ],
   "source": [
    "fractions=[8/10, 9/10]\n",
    "db = sq.Seq2SeqDataPreparer(inputs, targets, fractions)\n",
    "print(db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check data loader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    1\ttorch.Size([78, 64])\n",
      "    2\ttorch.Size([157, 64])\n",
      "    3\ttorch.Size([46, 64])\n",
      "    4\ttorch.Size([184, 64])\n",
      "    5\ttorch.Size([64, 64])\n",
      "    6\ttorch.Size([22, 64])\n",
      "    7\ttorch.Size([143, 64])\n",
      "    8\ttorch.Size([89, 64])\n",
      "    9\ttorch.Size([48, 64])\n",
      "   10\ttorch.Size([67, 64])\n",
      "   11\ttorch.Size([29, 64])\n",
      "   12\ttorch.Size([9, 64])\n",
      "   13\ttorch.Size([108, 64])\n",
      "   14\ttorch.Size([48, 64])\n",
      "   15\ttorch.Size([118, 64])\n",
      "   16\ttorch.Size([125, 64])\n",
      "   17\ttorch.Size([106, 64])\n",
      "   18\ttorch.Size([97, 64])\n",
      "   19\ttorch.Size([30, 64])\n",
      "   20\ttorch.Size([61, 64])\n",
      "   21\ttorch.Size([35, 64])\n",
      "   22\ttorch.Size([38, 64])\n",
      "   23\ttorch.Size([48, 64])\n",
      "   24\ttorch.Size([26, 64])\n",
      "   25\ttorch.Size([111, 64])\n",
      "   26\ttorch.Size([85, 64])\n",
      "   27\ttorch.Size([99, 64])\n",
      "   28\ttorch.Size([67, 64])\n",
      "   29\ttorch.Size([43, 64])\n",
      "   30\ttorch.Size([128, 64])\n",
      "   31\ttorch.Size([66, 64])\n",
      "   32\ttorch.Size([19, 64])\n",
      "   33\ttorch.Size([42, 64])\n",
      "   34\ttorch.Size([80, 64])\n",
      "   35\ttorch.Size([31, 64])\n",
      "   36\ttorch.Size([45, 64])\n",
      "   37\ttorch.Size([82, 64])\n",
      "   38\ttorch.Size([25, 64])\n",
      "   39\ttorch.Size([71, 64])\n",
      "   40\ttorch.Size([45, 64])\n",
      "   41\ttorch.Size([32, 64])\n",
      "   42\ttorch.Size([139, 64])\n",
      "   43\ttorch.Size([65, 64])\n",
      "   44\ttorch.Size([29, 64])\n",
      "   45\ttorch.Size([48, 64])\n",
      "   46\ttorch.Size([45, 64])\n",
      "   47\ttorch.Size([93, 64])\n",
      "   48\ttorch.Size([77, 64])\n",
      "   49\ttorch.Size([17, 64])\n",
      "   50\ttorch.Size([22, 64])\n",
      "   51\ttorch.Size([135, 64])\n",
      "   52\ttorch.Size([113, 64])\n",
      "   53\ttorch.Size([167, 64])\n",
      "   54\ttorch.Size([106, 64])\n",
      "   55\ttorch.Size([17, 64])\n",
      "   56\ttorch.Size([115, 64])\n",
      "   57\ttorch.Size([84, 64])\n",
      "   58\ttorch.Size([91, 64])\n",
      "   59\ttorch.Size([64, 64])\n",
      "   60\ttorch.Size([56, 64])\n",
      "   61\ttorch.Size([51, 64])\n",
      "   62\ttorch.Size([62, 64])\n",
      "   63\ttorch.Size([82, 64])\n",
      "   64\ttorch.Size([64, 64])\n",
      "   65\ttorch.Size([39, 64])\n",
      "   66\ttorch.Size([167, 64])\n",
      "   67\ttorch.Size([16, 64])\n",
      "   68\ttorch.Size([148, 64])\n",
      "   69\ttorch.Size([37, 64])\n",
      "   70\ttorch.Size([26, 64])\n",
      "   71\ttorch.Size([139, 64])\n",
      "   72\ttorch.Size([12, 64])\n",
      "   73\ttorch.Size([35, 64])\n",
      "   74\ttorch.Size([64, 64])\n",
      "   75\ttorch.Size([59, 64])\n",
      "   76\ttorch.Size([7, 64])\n",
      "   77\ttorch.Size([77, 64])\n",
      "   78\ttorch.Size([49, 64])\n",
      "   79\ttorch.Size([125, 64])\n",
      "   80\ttorch.Size([79, 64])\n",
      "   81\ttorch.Size([106, 64])\n",
      "   82\ttorch.Size([18, 64])\n",
      "   83\ttorch.Size([11, 64])\n",
      "   84\ttorch.Size([162, 64])\n",
      "   85\ttorch.Size([115, 64])\n",
      "   86\ttorch.Size([17, 64])\n",
      "   87\ttorch.Size([64, 64])\n",
      "   88\ttorch.Size([71, 64])\n",
      "   89\ttorch.Size([31, 64])\n",
      "   90\ttorch.Size([72, 64])\n",
      "   91\ttorch.Size([49, 64])\n",
      "   92\ttorch.Size([65, 64])\n",
      "   93\ttorch.Size([125, 64])\n",
      "   94\ttorch.Size([67, 64])\n",
      "   95\ttorch.Size([175, 64])\n",
      "   96\ttorch.Size([29, 64])\n",
      "   97\ttorch.Size([115, 64])\n",
      "   98\ttorch.Size([39, 64])\n",
      "   99\ttorch.Size([33, 64])\n",
      "  100\ttorch.Size([37, 64])\n"
     ]
    }
   ],
   "source": [
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "train_loader = sq.Seq2SeqDataLoader(db.train_data, device)\n",
    "\n",
    "for i, (X, Y) in enumerate(train_loader):\n",
    "    print('%5d\\t%s' % (i+1, Y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "seq2seq_data_generator.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
